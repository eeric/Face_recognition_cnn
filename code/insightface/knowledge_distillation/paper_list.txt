1.bake: Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification
2.reviewkd: Distilling Knowledge via Knowledge Review
3.Knowledge distillation: A good teacher is patient and consistent - 2106.05237
4.LD: Localization Distillation for Object Detection
5.EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation-2109.07222
6.Mask-invariant Face Recognition through Template-level Knowledge Distillation-2112.05646
7.Teacher-Student Training and Triplet Loss to Reduce the Effect of Drastic Face Occlusion-2111.10561
8.KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation
9.AT: Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer-1612.03928
10.RKD: Relational Knowledge Distillation-1904.05068
11.Decoupled Knowledge Distillation-2203.08679
12.SimKD: Knowledge Distillation with the Reused Teacher Classifier-2203.14001v1
13.Contrastive Representation Distillation-1910.10699
14.(CoupleFace)Relation Matters for Face Recognition Distillation-2204.05502
15.Masked Generative Distillation-2205.01529
16.Matching Guided Distillation-2008.09958
17.Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation-2205.14141
18.(Face2FaceœÅ)Real-Time High-Resolution One-Shot Face Reenactment
19.Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation
20.A Fast Knowledge Distillation Framework for Visual Recognition-2112.01528
21.Factorizing Knowledge in Neural Networks-2207.03337






